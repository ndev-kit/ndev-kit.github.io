{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Overview","text":""},{"location":"#napari-ndev-ndev","title":"napari-ndev (nDev)","text":"<p>A collection of widgets intended to serve any person seeking to process microscopy images from start to finish, with no coding necessary. <code>napari-ndev</code> was designed to address the gap between the napari viewer and batch python scripting.</p> <ul> <li>Accepts diverse image formats, dimensionality, file size, and maintains key metadata.</li> <li>Allows advanced, arbitrary image processing workflows to be used by novices.</li> <li>User-friendly sparse annotation and batch training of machine learning classifiers.</li> <li>Flexible label measurements, parsing of metadata, and summarization for easily readable datasets.</li> <li>Designed for ease of use, modification, and reproducibility.</li> </ul> <p>Check out the poster presented at BINA 2024 for an overview of the plugins in action!</p> <p>Also check out the Virtual I2K 2024 Workshop for an interactive tutorial to learn more!</p>"},{"location":"#plugin-overview","title":"Plugin Overview","text":"<ol> <li>Image Utilities: Intended for high-throuput image-labeling and management, while passing down important metadata. Allows opening image files and displaying in napari. Also reads metadata and allows customization prior to saving images and labels layers. Allows concatenation of image files and image layers for saving new images. Speeds up annotation by saving corresponding images and labels in designated folders. Also allows saving of shapes layers as labels in case shapes are being used as a region of interest.</li> <li>Workflow Widget: Batch pre-processing/processing images using napari-workflows.</li> <li>APOC Widget: Utilizes the excellent accelerated-pixel-and-object-classification (apoc) in a similar fashion to napari-apoc, but intended for batch training and prediction with a napari widget instead of scripting.<ul> <li>Custom Feature Set Widget: Generate a featureset to be used with the APOC widget. Also allows quick application in the napari viewer to an image layer to see all the features.</li> </ul> </li> <li>Measure Widget: Batch measurement of a label with optional corresponding image, label, and regions (ROIs) that can be used as an intensity image. Currently passed to <code>scikit-image.measure.regionprops</code>.</li> </ol> <p>The wide breadth of this plugin's scope is only made possible by the amazing libraries and plugins from the python and napari community, especially Robert Haase.</p>"},{"location":"BINA_poster/","title":"Poster","text":""},{"location":"BINA_poster/#poster-and-slide-from-bina-2024-in-madison-wi","title":"Poster and slide from BINA 2024 in Madison, WI","text":""},{"location":"beginner_setup/","title":"Beginner Setup","text":""},{"location":"beginner_setup/#setup-guide-to-napari-and-napari-ndev-for-beginners","title":"Setup Guide to napari and napari-ndev for beginners","text":"<p>This guide is intended for those who do not want to work with the command line and instead get started with a more 'traditional' installation approach. Note that the napari developers do not currently recommend this approach, and you may run in to unforeseen limitations. However, napari-ndev is designed as a comprehensive package, so it should work fairly well with the napari installer.</p>"},{"location":"beginner_setup/#napari-bundled-installer","title":"napari bundled installer","text":"<p>Download the latest version of napari found in the Assets at the bottom of the page. See the napari bundled app page for detailed instructions. Currently, napari is around 500MB to install.</p>"},{"location":"beginner_setup/#opening-napari","title":"opening napari","text":"<p>In your operating system, you can search for <code>napari</code> and open the version that you just installed. If you followed the default installation, you will also likely have a shortcut to napari on your desktop, e.g. <code>napari (0.5.4)</code>.</p>"},{"location":"beginner_setup/#napari-ndev-minimal-installation-from-ui","title":"napari-ndev minimal installation from UI","text":"<p>Inside napari, navigate to <code>Plugins</code> --&gt; <code>Install/Uninstall Plugins...</code>. In the <code>filter...</code> text box, search for <code>napari-ndev</code>. Select <code>PyPI</code> as the Source and <code>Install</code> the most recent (default) version. This is a complete version of <code>napari-ndev</code> and currently support full integration with other tools such as the <code>napari-assistant</code> and <code>napari-workflows</code>. This minimal installation is sufficient for full reproducibility of any analysis. Note: You must close the little black <code>When installing/uninstalling npe2...</code> pop-up for the installation to look to proceed. This installation will potentially take a few minutes depending on your computer and internet speed.</p>"},{"location":"beginner_setup/#optional-napari-ndev-full-installation","title":"[Optional] napari-ndev full installation","text":"<p>This method only installs python libraries that are BSD3 licensed. Some image formats (such as czi and lif) are not available unless installing the full nDev package. To install extra plugins go to the integrated <code>napari console</code> (lower left button in UI) and enter <code>!pip install napari-ndev[all]</code>. Note: napari will freeze when you enter this command; please be patient as it downloads and sets up the remaining packages. Restart napari for changes to take affect.</p>"},{"location":"beginner_setup/#opening-widgets","title":"opening widgets","text":"<p>Navigate to <code>Plugins</code> --&gt; <code>nDev</code> --&gt; Click any widget. Note: Something (I believe napari) appears to be currently bugged and requires that the first widget be opened twice; the first time it will give a warning, the second time it should open.</p>"},{"location":"beginner_setup/#bio-formats-support","title":"bio-formats support","text":"<p>After the full installation, you may still be missing support for some image formats. To support all image formats you may find it useful to install bioformats support via bioio-bioformats. To install enter into the console <code>!pip install bioio-bioformats</code>. You may also need to install java, to do so: <code>!conda install scyjava</code></p>"},{"location":"installation/","title":"Installation","text":""},{"location":"installation/#installation","title":"Installation","text":"<p>If you are unfamiliar with python or the command line, instead use the bundled app installer as demonstrated in Beginner Setup.</p>"},{"location":"installation/#install-with-uv","title":"Install with uv","text":"<p>uv is the newest and fastest way to manage python libraries. It is very easy to install, and simplifies environment manage, but requires some minimal input to the command line.  Install uv from here. Then, the simplest way to install <code>napari-ndev</code>:</p> <pre><code>uv tool install napari-ndev[qtpy-backend]\n</code></pre> <p>Alternatively, download the full opinionated package, which includes non-BSD3 licensed libraries with:</p> <pre><code>uv tool install napari-ndev[all]\n</code></pre> <p>Then, you can easily open napari with the command line every time by just typing:</p> <pre><code>napari-ndev\n</code></pre> <p>The tool version of <code>napari-ndev</code> effectively runs as an alias for <code>napari -w napari-ndev</code> and opens the <code>nDev App</code> upon launch. With this method, additional plugins installed via the plugin menu persist between each call to <code>napari-ndev</code></p> <p>To update a tool with uv:</p> <pre><code>uv tool upgrade napari-ndev\n</code></pre>"},{"location":"installation/#install-with-pip","title":"Install with pip","text":"<p>napari-ndev is a pure Python package, and can be installed with [pip] (and it is recommended to do so in a managed environment):</p> <pre><code>pip install napari-ndev\n</code></pre> <p>If napari is currently not installed in your environment, you will also need to include a QtPy backend:</p> <pre><code>pip install napari-ndev[qtpy-backend]\n</code></pre> <p>The easiest way to get started with napari-ndev is to install all the optional dependencies (see note below) with:</p> <pre><code>pip install napari-ndev[all]\n</code></pre> <p>Afterwards, you can call from the command line (in the same environment) <code>napari-ndev</code> to open napari with the <code>nDev App</code> open on launch.</p>"},{"location":"installation/#optional-libraries","title":"Optional Libraries","text":"<p>napari-ndev is most useful when interacting with some other napari plugins (e.g. napari-assistant) and can read additional filetypes. A few extra BSD3 compatible napari-plugins may be installed with [pip]:</p> <pre><code>pip install napari-ndev[extras]\n</code></pre> <p>napari-ndev can optionally use GPL-3 licensed libraries to enhance its functionality, but are not required. If you choose to install and use these optional dependencies, you must comply with the GPL-3 license terms. The main functional improvement is from some <code>bioio</code> libraries to support extra image formats, including <code>czi</code> and <code>lif</code> files. These libraries can be installed with [pip]:</p> <pre><code>pip install napari-ndev[gpl-extras]\n</code></pre> <p>In addition, you may need to install specific <code>bioio</code> readers to support your specific image, such as <code>bioio-czi</code> and <code>bioio-lif</code> (included in <code>[gpl-extras]</code>) or <code>bioio-bioformats</code> (which needs conda installed).</p>"},{"location":"installation/#development-libraries","title":"Development Libraries","text":"<p>For development use the <code>[dev]</code> optional libraries to verify your changes, which includes the <code>[docs]</code> and <code>[testing]</code> optional groups. However, the Github-CI will test pull requests with <code>[testing]</code> only.</p>"},{"location":"installation/#development-with-uv","title":"Development with uv","text":"<p>uv can be a useful tool for building as similar an environment as possible across systems. To do so, navigate in your terminal to the <code>napari-ndev</code> source directory. <code>--python</code> sets the minimum python version. <code>--no-workspace</code> prevents discovering parent workspaces. Then:</p> <pre><code>uv init --python 3.11 --no-workspace\nuv sync\n</code></pre> <p>You may use uv to set a certain python version, e.g.:</p> <pre><code>uv pin python 3.11\n</code></pre> <p>To use uv to install extras (like with <code>napari-ndev[dev]</code>), use:</p> <pre><code>uv sync --extra dev\n</code></pre> <p>You may also test the tool version of uv during development with:</p> <pre><code>uv install tool .\n</code></pre> <p>You can also test with tox in parallel (via tox-uv) with:</p> <pre><code>tox - p auto\n</code></pre>"},{"location":"widget_further_info/","title":"Further Widget Info","text":""},{"location":"widget_further_info/#further-info","title":"Further Info","text":""},{"location":"widget_further_info/#1-image-utilities","title":"1. Image Utilities","text":"<p>A quick and easy way to save annotations (a napari labels layer) and corresponding images to corresponding folders. Best if the images are opened with the <code>nDev</code> reader (using bioio under the hood) -- which can be as simple as drag and drop opening by setting the appropriate default reader for each file type in Preferences -&gt; Plugins--in order to utilize the metadata present for saving the image-label pairs.</p> <p>Quick uniform adjustments to a folder of images, saving the output. Currently supports selecting channels, slicing Z, cropping/downsampling in XY, and doing a max projection of the sliced/cropped image data. To be added: alternative projection types, slicing in T, and compatibility with non TCZYX images (but this is not a priority since bioio currently always extracts images as TCZYX even if a dim is only length 1.</p>"},{"location":"widget_further_info/#2-workflow-widget","title":"2. Workflow Widget","text":"<p>Batch pre-processing/processing images using napari-workflows.  Images are processed outside the napari-viewer using bioio as both reader and writer. Prior to passing the images to napari-workflows, the user selects the correct images as the roots (inputs) and thus napari-workflows matches the processing to create the outputs. The advantage of using napari-workflows for batch processing is that it provides an incredibly flexible processing interface without writing a novel widget for small changes to processing steps like specific filters, segmentation, or measurements. Currently only intended for use with images as inputs and images as outputs from napari-workflows, though there is future potential to have other outputs possible, such as .csv measurement arrays.</p>"},{"location":"widget_further_info/#3-apoc-widget","title":"3. APOC Widget","text":"<p>Utilizes the excellent accelerated-pixel-and-object-classification (apoc) in a similar fashion to napari-apoc, but intended for batch training and prediction with a napari widget instead of scripting. Recognizes pre established feature set, and custom feature sets (a string of filters and radii) can be generated with a corresponding widget. Also contains a Custom Feature Set widget which allows application of all the features to a layer in the viewer, for improved visualization.</p>"},{"location":"widget_further_info/#4-measure-widget","title":"4. Measure Widget","text":"<p>Batch measurements using scikit-image's regionprops. This can measure features of a label such as area, eccentricity, and more but also can measure various intensity metrics. Attempts to support post-processing of measurements, grouping, and more to make downstream analyses easier for users. Will be updated in the future to include nyxus.</p>"},{"location":"examples/skimage_workflow/","title":"Skimage workflow","text":"In\u00a0[43]: Copied! <pre>import napari_segment_blobs_and_things_with_membranes as nsbatwm\nimport numpy as np\nimport stackview\nfrom napari_workflows import Workflow\nfrom napari_workflows._io_yaml_v1 import load_workflow, save_workflow\n\nfrom napari_ndev import nImage\n</pre> import napari_segment_blobs_and_things_with_membranes as nsbatwm import numpy as np import stackview from napari_workflows import Workflow from napari_workflows._io_yaml_v1 import load_workflow, save_workflow  from napari_ndev import nImage  In\u00a0[53]: Copied! <pre>wf = Workflow()\n\nwf.set('membrane-gb', nsbatwm.gaussian_blur, 'membrane', sigma=1)\nwf.set('membrane-threshold', nsbatwm.threshold_otsu, 'membrane-gb')\nwf.set('membrane-label', nsbatwm.label, 'membrane-threshold')\n\nwf.set('nucleus-gb', nsbatwm.gaussian_blur, 'nucleus', sigma=1)\nwf.set('nucleus-threshold', nsbatwm.threshold_otsu, 'nucleus-gb')\nwf.set('nucleus-label', nsbatwm.label, 'nucleus-threshold')\n\nsave_workflow('cpu_workflow-2roots-2leafs.yaml', wf)\n</pre> wf = Workflow()  wf.set('membrane-gb', nsbatwm.gaussian_blur, 'membrane', sigma=1) wf.set('membrane-threshold', nsbatwm.threshold_otsu, 'membrane-gb') wf.set('membrane-label', nsbatwm.label, 'membrane-threshold')  wf.set('nucleus-gb', nsbatwm.gaussian_blur, 'nucleus', sigma=1) wf.set('nucleus-threshold', nsbatwm.threshold_otsu, 'nucleus-gb') wf.set('nucleus-label', nsbatwm.label, 'nucleus-threshold')  save_workflow('cpu_workflow-2roots-2leafs.yaml', wf)  In\u00a0[54]: Copied! <pre>wf = load_workflow('cpu_workflow-2roots-2leafs.yaml')\n\nimg = nImage(r'images\\cells3d2ch.tiff')\nmembrane = img.get_image_data('TCZYX', C=0)\nmembrane = np.squeeze(membrane)\n\nnuclei = img.get_image_data('TCZYX', C=1)\nnuclei = np.squeeze(nuclei)\n\nwf.set('membrane', membrane)\nwf.set('nucleus', nuclei)\nmembrane_label = wf.get('nucleus-label')\n\nstackview.imshow(membrane_label)\n</pre> wf = load_workflow('cpu_workflow-2roots-2leafs.yaml')  img = nImage(r'images\\cells3d2ch.tiff') membrane = img.get_image_data('TCZYX', C=0) membrane = np.squeeze(membrane)  nuclei = img.get_image_data('TCZYX', C=1) nuclei = np.squeeze(nuclei)  wf.set('membrane', membrane) wf.set('nucleus', nuclei) membrane_label = wf.get('nucleus-label')  stackview.imshow(membrane_label) In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"examples/measure/measure_widget/","title":"Measure Widget","text":"In\u00a0[3]: Copied! <pre>import napari\nfrom napari.utils import nbscreenshot\n\nviewer = napari.Viewer()\nviewer.window.resize(1000,700) # w x h\nviewer.window.add_plugin_dock_widget('napari-ndev', 'Measure Widget')\nnbscreenshot(viewer)\n</pre> import napari from napari.utils import nbscreenshot  viewer = napari.Viewer() viewer.window.resize(1000,700) # w x h viewer.window.add_plugin_dock_widget('napari-ndev', 'Measure Widget') nbscreenshot(viewer) <pre>WARNING: QWindowsWindow::setGeometry: Unable to set geometry 1920x1310+1280+550 (frame: 1942x1366+1269+505) on QWidgetWindow/\"_QtMainWindowClassWindow\" on \"\\\\.\\DISPLAY1\". Resulting geometry: 2882x1968+1283+564 (frame: 2904x2024+1272+519) margins: 11, 45, 11, 11 minimum size: 385x492 MINMAXINFO maxSize=0,0 maxpos=0,0 mintrack=792,1040 maxtrack=0,0)\n21-Sep-24 11:32:12 - vispy    - WARNING  - QWindowsWindow::setGeometry: Unable to set geometry 1920x1310+1280+550 (frame: 1942x1366+1269+505) on QWidgetWindow/\"_QtMainWindowClassWindow\" on \"\\\\.\\DISPLAY1\". Resulting geometry: 2882x1968+1283+564 (frame: 2904x2024+1272+519) margins: 11, 45, 11, 11 minimum size: 385x492 MINMAXINFO maxSize=0,0 maxpos=0,0 mintrack=792,1040 maxtrack=0,0)\n</pre> Out[3]: In\u00a0[4]: Copied! <pre>nbscreenshot(viewer)\n</pre> nbscreenshot(viewer) Out[4]: In\u00a0[5]: Copied! <pre>nbscreenshot(viewer)\n</pre> nbscreenshot(viewer) Out[5]: In\u00a0[6]: Copied! <pre>nbscreenshot(viewer)\n</pre> nbscreenshot(viewer) Out[6]: In\u00a0[7]: Copied! <pre>import pandas as pd\n\nraw_data = pd.read_csv(r'./data\\measure_props_Morphology.csv')\ndisplay(raw_data.shape)\nraw_data.head()\n</pre> import pandas as pd  raw_data = pd.read_csv(r'./data\\measure_props_Morphology.csv') display(raw_data.shape) raw_data.head() <pre>(1263, 12)</pre> Out[7]: id date HIC well scene label area intensity_max-DAPI Class row column chelation media 0 2024-08-07 25x 24HIC NCOA4 647 FT 568 PHALL 48... 2024-08-07 24 H9 P1-H9 1 0.600632 0.0 H 9 100uM DFO NGM 1 2024-08-07 25x 24HIC NCOA4 647 FT 568 PHALL 48... 2024-08-07 24 H9 P1-H9 2 0.246413 0.0 H 9 100uM DFO NGM 2 2024-08-07 25x 24HIC NCOA4 647 FT 568 PHALL 48... 2024-08-07 24 H9 P1-H9 3 3.203368 0.0 H 9 100uM DFO NGM 3 2024-08-07 25x 24HIC NCOA4 647 FT 568 PHALL 48... 2024-08-07 24 H9 P1-H9 4 0.308016 0.0 H 9 100uM DFO NGM 4 2024-08-07 25x 24HIC NCOA4 647 FT 568 PHALL 48... 2024-08-07 24 H9 P1-H9 5 269.283163 1.0 H 9 100uM DFO NGM In\u00a0[8]: Copied! <pre>nbscreenshot(viewer)\n</pre> nbscreenshot(viewer) Out[8]: In\u00a0[9]: Copied! <pre>grouped_data = pd.read_csv(r'./data\\measure_props_Morphology_grouped.csv')\ndisplay(grouped_data.shape)\ngrouped_data.head()\n</pre> grouped_data = pd.read_csv(r'./data\\measure_props_Morphology_grouped.csv') display(grouped_data.shape) grouped_data.head() <pre>(25, 2)</pre> Out[9]: id label_count 0 2024-08-07 25x 24HIC NCOA4 647 FT 568 PHALL 48... 16 1 2024-08-07 25x 24HIC NCOA4 647 FT 568 PHALL 48... 14 2 2024-08-07 25x 24HIC NCOA4 647 FT 568 PHALL 48... 40 3 2024-08-07 25x 24HIC NCOA4 647 FT 568 PHALL 48... 20 4 2024-08-07 25x 24HIC NCOA4 647 FT 568 PHALL 48... 14 In\u00a0[10]: Copied! <pre>nbscreenshot(viewer)\n</pre> nbscreenshot(viewer) Out[10]: In\u00a0[12]: Copied! <pre>grouped_data = pd.read_csv(r'./data\\measure_props_Morphology_grouped.csv')\ndisplay(grouped_data.shape)\ngrouped_data\n</pre> grouped_data = pd.read_csv(r'./data\\measure_props_Morphology_grouped.csv') display(grouped_data.shape) grouped_data <pre>(47, 8)</pre> Out[12]: id date HIC well scene intensity_max-DAPI Class label_count area_mean 0 2024-08-07 25x 24HIC NCOA4 647 FT 568 PHALL 48... 2024-08-07 24 H9 P1-H9 0.0 15 9.641934 1 2024-08-07 25x 24HIC NCOA4 647 FT 568 PHALL 48... 2024-08-07 24 H9 P1-H9 1.0 1 269.283163 2 2024-08-07 25x 24HIC NCOA4 647 FT 568 PHALL 48... 2024-08-07 24 B9 P8-B9 0.0 12 1.036988 3 2024-08-07 25x 24HIC NCOA4 647 FT 568 PHALL 48... 2024-08-07 24 B9 P8-B9 1.0 2 461.962697 4 2024-08-07 25x 24HIC NCOA4 647 FT 568 PHALL 48... 2024-08-07 24 C9 P8-C9 0.0 39 0.576938 5 2024-08-07 25x 24HIC NCOA4 647 FT 568 PHALL 48... 2024-08-07 24 C9 P8-C9 1.0 1 297.451244 6 2024-08-07 25x 24HIC NCOA4 647 FT 568 PHALL 48... 2024-08-07 24 A9 P4-A9 0.0 18 3.002302 7 2024-08-07 25x 24HIC NCOA4 647 FT 568 PHALL 48... 2024-08-07 24 A9 P4-A9 1.0 2 446.762097 8 2024-08-07 25x 24HIC NCOA4 647 FT 568 PHALL 48... 2024-08-07 24 D9 P8-D9 0.0 12 6.541494 9 2024-08-07 25x 24HIC NCOA4 647 FT 568 PHALL 48... 2024-08-07 24 D9 P8-D9 1.0 2 439.747028 10 2024-08-07 25x 24HIC NCOA4 647 FT 568 PHALL 48... 2024-08-07 24 E9 P4-E9 0.0 27 1.536659 11 2024-08-07 25x 24HIC NCOA4 647 FT 568 PHALL 48... 2024-08-07 24 E9 P4-E9 1.0 2 438.938486 12 2024-08-07 25x 24HIC NCOA4 647 FT 568 PHALL 48... 2024-08-07 24 E9 P3-E9 0.0 8 10.378221 13 2024-08-07 25x 24HIC NCOA4 647 FT 568 PHALL 48... 2024-08-07 24 E9 P3-E9 1.0 2 414.428097 14 2024-08-07 25x 24HIC NCOA4 647 FT 568 PHALL 48... 2024-08-07 24 G9 P8-G9 0.0 55 0.816803 15 2024-08-07 25x 24HIC NCOA4 647 FT 568 PHALL 48... 2024-08-07 24 G9 P8-G9 1.0 2 407.266720 16 2024-08-07 25x 48HIC NCOA4 647 FT 568 PHALL 48... 2024-08-07 48 H9 P20-H9 0.0 34 1.123353 17 2024-08-07 25x 48HIC NCOA4 647 FT 568 PHALL 48... 2024-08-07 48 H9 P20-H9 1.0 2 360.717772 18 2024-08-07 25x 48HIC NCOA4 647 FT 568 PHALL 48... 2024-08-07 48 C9 P6-C9 0.0 34 5.815165 19 2024-08-07 25x 48HIC NCOA4 647 FT 568 PHALL 48... 2024-08-07 48 C9 P6-C9 1.0 3 355.055407 20 2024-08-07 25x 48HIC NCOA4 647 FT 568 PHALL 48... 2024-08-07 48 C9 P7-C9 0.0 16 4.552864 21 2024-08-07 25x 48HIC NCOA4 647 FT 568 PHALL 48... 2024-08-07 48 C9 P7-C9 1.0 2 503.483281 22 2024-08-07 25x 48HIC NCOA4 647 FT 568 PHALL 48... 2024-08-07 48 D9 P6-D9 0.0 59 0.548948 23 2024-08-07 25x 48HIC NCOA4 647 FT 568 PHALL 48... 2024-08-07 48 D9 P6-D9 1.0 4 404.764088 24 2024-08-07 25x 48HIC NCOA4 647 FT 568 PHALL 48... 2024-08-07 48 D9 P10-D9 0.0 54 0.568119 25 2024-08-07 25x 48HIC NCOA4 647 FT 568 PHALL 48... 2024-08-07 48 D9 P10-D9 1.0 2 439.947239 26 2024-08-07 25x 48HIC NCOA4 647 FT 568 PHALL 48... 2024-08-07 48 E9 P7-E9 0.0 52 1.061767 27 2024-08-07 25x 48HIC NCOA4 647 FT 568 PHALL 48... 2024-08-07 48 E9 P7-E9 1.0 4 573.364456 28 2024-08-07 25x 48HIC NCOA4 647 FT 568 PHALL 48... 2024-08-07 48 F9 P11-F9 0.0 201 13.183400 29 2024-08-07 25x 48HIC NCOA4 647 FT 568 PHALL 48... 2024-08-07 48 F9 P11-F9 1.0 2 824.220550 30 2024-08-07 25x 72HIC NCOA4 647 FT 568 PHALL 48... 2024-08-07 72 H9 P4-H9 0.0 44 0.557229 31 2024-08-07 25x 72HIC NCOA4 647 FT 568 PHALL 48... 2024-08-07 72 H9 P4-H9 1.0 1 517.975443 32 2024-08-07 25x 72HIC NCOA4 647 FT 568 PHALL 48... 2024-08-07 72 B9 P6-B9 0.0 68 2.446237 33 2024-08-07 25x 72HIC NCOA4 647 FT 568 PHALL 48... 2024-08-07 72 B9 P6-B9 1.0 1 656.428725 34 2024-08-07 25x 72HIC NCOA4 647 FT 568 PHALL 48... 2024-08-07 72 B9 P8-B9 0.0 64 0.430741 35 2024-08-07 25x 72HIC NCOA4 647 FT 568 PHALL 48... 2024-08-07 72 B9 P8-B9 1.0 1 602.941711 36 2024-08-07 25x 72HIC NCOA4 647 FT 568 PHALL 48... 2024-08-07 72 B9 P7-B9 0.0 73 0.641138 37 2024-08-07 25x 72HIC NCOA4 647 FT 568 PHALL 48... 2024-08-07 72 B9 P7-B9 1.0 1 1369.917450 38 2024-08-07 25x 72HIC NCOA4 647 FT 568 PHALL 48... 2024-08-07 72 A9 P3-A9 0.0 48 1.359763 39 2024-08-07 25x 72HIC NCOA4 647 FT 568 PHALL 48... 2024-08-07 72 D9 P7-D9 0.0 26 31.638595 40 2024-08-07 25x 72HIC NCOA4 647 FT 568 PHALL 48... 2024-08-07 72 D9 P7-D9 1.0 1 443.882146 41 2024-08-07 25x 72HIC NCOA4 647 FT 568 PHALL 48... 2024-08-07 72 E9 P2-E9 0.0 7 1.095658 42 2024-08-07 25x 72HIC NCOA4 647 FT 568 PHALL 48... 2024-08-07 72 E9 P2-E9 1.0 2 446.831401 43 2024-08-07 25x 72HIC NCOA4 647 FT 568 PHALL 48... 2024-08-07 72 F9 P13-F9 0.0 36 0.533039 44 2024-08-07 25x 72HIC NCOA4 647 FT 568 PHALL 48... 2024-08-07 72 F9 P13-F9 1.0 1 736.743949 45 2024-08-07 25x 72HIC NCOA4 647 FT 568 PHALL 48... 2024-08-07 72 G9 P9-G9 0.0 56 5.597644 46 2024-08-07 25x 72HIC NCOA4 647 FT 568 PHALL 48... 2024-08-07 72 G9 P9-G9 1.0 2 789.691934"},{"location":"examples/measure/measure_widget/#measure-widget","title":"Measure Widget\u00b6","text":"<p>This page describes how to generate data outputs from measuring label images. Currently, labels are measured with <code>scikit-image.regionprops</code> and exported to a <code>.csv</code> file (which can be opened in any spreadsheet application or stats program). In addition, the user can specify two different sets of metadata to add additional information: 1) <code>ID Regex</code> which will parse information currently from the filename and the scene information; an example will be shown in <code>Example ID String</code>. 2) <code>Tx Map</code> can be used to add information based on a multi-well plate map. The <code>ID</code> mapped to is any column that can be created by <code>ID Regex</code> or the <code>Scene</code> information.</p> <p>In some ways, this widget is knowingly complex, however, it is certainly intended that a more advanced user can provide the proper <code>ID Regex</code> and <code>Tx Map</code> for a user for the experiments, and in the end create a summarized dataset. While the measure widget initially spits out a raw data file which will contain more rows than most datasets you are familiar with, since it is in 'long' format. In other words, each row represents a single object.</p> <p>Thus, to summarize your data on any measure of summary that you would like, use the <code>Grouping</code> tab and select the unique identifiers to summarize by. You will then select a column to <code>Count</code> which will tell you the number of labels in that group. Optionally, you can add <code>Aggregation Columns</code> to summarize all selected columns by the selected aggregation functions. For example, you may wish to know the <code>mean area</code> of all objects in your groups.</p>"},{"location":"examples/measure/measure_widget/#example-folder","title":"Example folder\u00b6","text":"<p>The only required directory is the <code>label directory</code> whereby the objects that you want to measure are found. You should also select/make an <code>output directory</code> so that you know where your file is saved.</p> <p>Once you select the <code>label directory</code>, label names will populate both the <code>label image</code> and <code>intensity images</code>. The only required selection is a <code>label image</code> once this is done, you can hit the <code>Measure</code> button and will get the most minimal dataset possible. If you would like to measure multiple <code>labels</code> at the same time, then select multiple labels and it will iterate through each label channel in the images.</p> <p>However, you can also add other intensity images (by loading an image or region directory, the reasoning for these namings will come in future tutorials) and select them in the widget to be measured against. Then, in <code>Region Props</code> tab you can select additional properties to measure.</p> <p>In this example I want to measure the <code>intensity max</code> of the corresponding <code>Labels: DAPI Class</code> because this will give me the 'type' of DAPI that is inside each <code>Morphology</code> object. This is because the background is 0, live is 1, and dead is 2. So, we can later filter by the 'DAPI Class' of each morphology object. In other words, intensity images don't have to be raw intensity values, but other labels can be used.</p>"},{"location":"examples/measure/measure_widget/#id-regex","title":"ID Regex\u00b6","text":"<p>In this example, one such ID string is: <code>'2024-08-07 25x 24HIC NCOA4 647 FT 568 PHALL 488 DAPI OBL_107_106_P1-H9.ome__0__Image:0'</code> From here, I can extract multiple different bits of information, which is why saving interesting metadata into filenames can be useful. This needs to be a dictionary, where each key represents a column, and the value for that key is the regex (regular expression) pattern used to extract that information. The only quirk (besides regex) is that there must be at least 1 'group' aka a pattern surrounded by parenetheses. This pattern surrounded by parentheses is what will be saved into the column. As such, there can be extra regex that isn't kept, but can be used to locate the pattern.</p> <pre><code>{\n    'scene': r'(P\\d{1,3}-\\w+).ome',\n    'well': r'-(\\w+).ome',\n    'HIC': r'(\\d{1,3})HIC',\n    'date': r'(\\d{4}-\\d{2}-\\d{2})',\n}\n</code></pre>"},{"location":"examples/measure/measure_widget/#tx-map","title":"Tx Map\u00b6","text":"<p>This section uses <code>napari_ndev:PlateMapper</code>. It is currently only set up to be used with typical culture plate dimensions, but will hopefully be updated in the future to be flexible to arbitrary patterns, so that a <code>Treatment ID</code> can include something like slide or section information that can then be mapped to treatments or positions of some kind. First, press the <code>Update Treatment ID Choices</code> to read possibilities from the <code>ID Regex</code> container. In the <code>treatment ID</code>, select the name of the column with the ID in it, which will usually be obtained from <code>ID Regex</code>, for this example it is 'well'.</p> <p>Then, select the number of wells for your plate, to automatically make a plate in the typical layout. For example, a 96-well plate would automatically map a A-H (8 row) plate with 12 columns.</p> <p>For now, you provide lists of strings with ranges representing wells on a plate. For <code>PlateMapper</code> provide a dictionary, where each key represents a column header, and then the value-dictionary has a key which is what will get mapped to the matching well-value. For example:</p> <pre><code>{\n  'chelation':{\n    'Control': ['B1:C12'],\n    '50uM DFP': ['D1:E12'],\n    '100uM DFP': ['F1:G12'],\n    '100uM DFO': ['A1:A12', 'H1:H12'],\n  },\n  'media':{\n    'NGM': ['A1:H12'],\n  }\n}\n</code></pre>"},{"location":"examples/measure/measure_widget/#finally-measure","title":"Finally, Measure\u00b6","text":"<p>At any point once minimal selections were made, you could click the <code>Measure</code> button and it will measure all the labels in batch, and save the results to the output directory.</p>"},{"location":"examples/measure/measure_widget/#grouping-data","title":"Grouping Data!\u00b6","text":"<p>After acquiring your dataset, you may be interested in processing it further with the <code>Grouping</code> tab. This will reduce it from many rows, to much fewer. This example before has 1263 rows and 12 columns.</p>"},{"location":"examples/measure/measure_widget/#using-the-grouping-tab","title":"Using the Grouping Tab\u00b6","text":"<p>Load in your raw data of interest, and it will populate all the possible column names. Minimally, select grouping columns and the count column. For example, if I want to group by every 'id' (i.e. filename, with scene info) I would be able to leave the default values.</p>"},{"location":"examples/measure/measure_widget/#advanced-grouping","title":"Advanced grouping\u00b6","text":"<p>However, you will note that now only the 'id' and 'label_count' columns are present. If I instead wanted to keep all that careful metadata I extracted earlier, I would also want to select other grouping data, such as id, date, HIC, well, and scene, and 'intensity_max-DAPI Class' which represents the type of cell present.</p> <p>And I could also select a column to aggregate, such as getting the 'mean' of the 'area' for each of these groups. Remember, if you keep the 'id' then minimally each file will get summarized. To have a more general summary (which would not typically be recommended), do not use 'id'.</p>"},{"location":"examples/measure/measure_widget/#interpreting-the-results","title":"Interpreting the results\u00b6","text":"<p>Now, you should be able to place this easily into your stats program of choice (mine is using Python!). Note how we have shown in the first image that there are 15 labels with a DAPI class of zero, meaning that this label does not contain a nucleus and there is 1 label with a DAPI Class of 1 (meaning that it is alive, based on a previous classifier employed with APOC Widget). This also shows for example that the <code>area_mean</code> is larger for an alive cell, compared to whatever debris there must be that doesn't have a nucleus.</p> <p>This is also useful for grouping your data to double check that the results intuitively make sense. You should also make sure to check through your labels. You can try <code>napari-ndev:ImageOverview</code> for that. It will be added to the widgets soon, to quickly have overview .png files to scroll through on any system.</p>"},{"location":"examples/utilities/image_utilities/","title":"Image Utilities","text":"In\u00a0[1]: Copied! <pre>import pathlib\n\nimport napari\nfrom napari.utils import nbscreenshot\n\nparent = pathlib.Path.cwd().parent\nrel_path = 'images/cropped_neuron.ome.tiff'\n\nviewer = napari.Viewer()\nviewer.window.resize(1500, 800) # w x h\nviewer.window.add_plugin_dock_widget('napari-ndev', 'Image Utilities')\nviewer.open(parent / rel_path, plugin='napari-bioio')\n\nnbscreenshot(viewer)\n</pre> import pathlib  import napari from napari.utils import nbscreenshot  parent = pathlib.Path.cwd().parent rel_path = 'images/cropped_neuron.ome.tiff'  viewer = napari.Viewer() viewer.window.resize(1500, 800) # w x h viewer.window.add_plugin_dock_widget('napari-ndev', 'Image Utilities') viewer.open(parent / rel_path, plugin='napari-bioio')  nbscreenshot(viewer) Out[1]: In\u00a0[2]: Copied! <pre>nbscreenshot(viewer)\n</pre> nbscreenshot(viewer) Out[2]: In\u00a0[3]: Copied! <pre>nbscreenshot(viewer)\n</pre> nbscreenshot(viewer) Out[3]: In\u00a0[4]: Copied! <pre>nbscreenshot(viewer)\n</pre> nbscreenshot(viewer) Out[4]:"},{"location":"examples/utilities/image_utilities/#image-utilities","title":"Image Utilities\u00b6","text":"<p>This example will describe how to manage metadata, crop, and annotate images with the Image Utilities widget. The widget can be opened from <code>Plugins --&gt; nDev --&gt; Image Utilities</code>. To crop layers in the viewer, open <code>Plugins --&gt; crop region (napari_crop)</code>. Below, we load napari and open the Image Utilities Widget and load in a large image originally saved with the widget from a multi-scene CZI file. Note that currently no metadata populates the widget because the file was not selected with <code>Select Files</code>. Instead, the image was opened with the <code>napari-bioio</code> plugin; however, the metadata is saved and could normally be accessed from <code>viewer.layers[n].metadata['bioimage']</code>.</p> <p>To access metadata from any image in the viewer, press the <code>Selected Layer</code> button in the <code>Update Metadata from:</code> box.</p>"},{"location":"examples/utilities/image_utilities/#update-metadata-from-layer","title":"Update Metadata from layer\u00b6","text":"<p>Now both Channel Name(s) and the Scale is shown</p>"},{"location":"examples/utilities/image_utilities/#updating-metadata-automatically-with-widget-file-selection","title":"Updating Metadata Automatically with Widget File Selection\u00b6","text":"<p>Select one or multiple files which could be opened into the viewer, but these files do not have to be in order to be saved.</p> <p>To turn off auto metadata update on file selection uncheck <code>Update Metadata</code>. You may open the image (which will attempt to use <code>napari-aicsimageio</code>, if it exists, otherwise a different compatible reader) or <code>Select Next</code> and it will iterate to the next alpha-numerically sorted filename(s) matching the current number that are open.</p>"},{"location":"examples/utilities/image_utilities/#concatenating-files-and-saving-layers","title":"Concatenating Files and Saving Layers\u00b6","text":"<p><code>Concatenate Files</code> checkbox below allows saving of image data as OME-TIFF without empty channels. This could be useful if your microscope saves images as RGB with empty channels. Alternatively, if <code>Concatenate Layers</code> is checked then it will combine any layers selected in the left panel <code>layer list</code> of napari. Having both checked will stack the select file path with any select viewer layers, which could be useful if you wanted to process images and then combine with the original.</p> <p>All metadata is passed from the widget to <code>bioio.writers.OmeTiffWriter</code> to keep metadata and is used by all other widgets of the plugin.</p>"},{"location":"examples/utilities/image_utilities/#quick-annotation","title":"Quick Annotation\u00b6","text":"<p>Both the Shapes and Labels layers can be quickly drawn on to your image to annotate in whatever way suits your needs. Then, when the layer you want is selected hit the button in the widget to save your image, layer, or shape (which will be converted to a scaled label). Each 'Shape' will also get a corresponding label, allowing multi-ROI consistency across images.</p>"},{"location":"examples/utilities/image_utilities/#utilizing-shapes-for-future-annotations","title":"Utilizing Shapes for future annotations\u00b6","text":"<p>In order to re-use Shapes for future annotations (akin to an ImageJ/FIJI ROI), go to <code>File -&gt; Save Selected Layer</code> and save the Shapes layer as a .CSV. When you load this .csv back into napari it will always load with a scale of (1.0,1.0,1.0) and you will thus need to select the shapes layer and <code>Scale Layer(s)</code> using the widget to re-size your annotation to the expected size.</p>"},{"location":"examples/workflow/workflow_napari-assistant/","title":"Generating workflows with napari","text":"In\u00a0[1]: Copied! <pre>import napari\nfrom napari.utils import nbscreenshot\n\nviewer = napari.Viewer()\nviewer.window.resize(1000, 700) # w x h\nviewer.window.add_plugin_dock_widget('napari-assistant')\nviewer.open_sample('napari', 'human_mitosis')\nnbscreenshot(viewer)\n</pre> import napari from napari.utils import nbscreenshot  viewer = napari.Viewer() viewer.window.resize(1000, 700) # w x h viewer.window.add_plugin_dock_widget('napari-assistant') viewer.open_sample('napari', 'human_mitosis') nbscreenshot(viewer) Out[1]: <pre>2024-09-15 23:13:22.581 | INFO     | napari_assistant._gui._category_widget:call_op:178 - gaussian_blur (clesperanto)(..., 1.0, 1.0, 0.0)\n2024-09-15 23:13:25.405 | INFO     | napari_assistant._gui._category_widget:call_op:178 - median_sphere (clesperanto)(..., 1.0, 1.0, 0.0)\n2024-09-15 23:13:26.324 | INFO     | napari_assistant._gui._category_widget:call_op:178 - top_hat_box (clesperanto)(..., 10.0, 10.0, 0.0)\n2024-09-15 23:13:27.501 | INFO     | napari_assistant._gui._category_widget:call_op:178 - voronoi_otsu_labeling (clesperanto)(..., 2.0, 2.0)\n</pre> In\u00a0[2]: Copied! <pre>nbscreenshot(viewer)\n</pre> nbscreenshot(viewer) Out[2]:"},{"location":"examples/workflow/workflow_napari-assistant/#generating-workflows-with-napari","title":"Generating workflows with napari\u00b6","text":"<p>This page describes how to generate custom workflows using napari using the calculator-like interface from <code>napari-assistant</code>. In many ways, this should be reminiscent of using the <code>Macro Recorder</code> in FIJI/ImageJ, but you will hopefully find it has more flexibility and advantages. Ultimately, our goal is to easily reproduce image processing steps with a <code>napari-workflows</code> <code>.yaml</code> file and utilize it for batch processing with <code>napari-ndev:Workflow Widget</code>.</p>"},{"location":"examples/workflow/workflow_napari-assistant/#napari-assistant","title":"napari-assistant\u00b6","text":"<p>In order to generate workflows with napari, you may like to <code>pip install napari-ndev[extra-plugins]</code> to install the napari-assistant and other cooperating plugins. You can then access the assistant via the <code>Plugins menu -&gt; Assistant (clesperanto)</code></p> <p>A thorough tutorial on how to use napari-assistant, including video, can be found here. The assistant is quite flexible and both functions and parameters are modifiable on-the-fly, but can be overall quirky at times.</p>"},{"location":"examples/workflow/workflow_napari-assistant/#apply-processing-steps","title":"Apply processing steps\u00b6","text":"<p>With the napari-assistant, I've selected the 'nuclei' layer and do a few processing steps to label the image. This is flexible for 3D images as well, because napari-workflow will save the 2D or 3D parameters as necessary.</p>"},{"location":"examples/workflow/workflow_napari-assistant/#save-the-workflow-file","title":"Save the workflow file\u00b6","text":"<p>Using the 2nd from the lower right button, <code>save and load workflows</code> -&gt; export workflow to file. I have named this <code>viewer-segment-nuclei-sample.yaml</code> into the resources folder in the docs library, but you should save it wherever you want to keep it in your project.</p>"},{"location":"examples/workflow/workflow_scripting/","title":"Scripting a Workflow","text":"In\u00a0[\u00a0]: Copied! <pre>import napari_segment_blobs_and_things_with_membranes as nsbatwm\nimport numpy as np\nimport stackview\nfrom napari_workflows import Workflow\nfrom napari_workflows._io_yaml_v1 import load_workflow, save_workflow\n\nfrom napari_ndev import nImage\n</pre> import napari_segment_blobs_and_things_with_membranes as nsbatwm import numpy as np import stackview from napari_workflows import Workflow from napari_workflows._io_yaml_v1 import load_workflow, save_workflow  from napari_ndev import nImage  In\u00a0[\u00a0]: Copied! <pre>wf = Workflow()\n\nwf.set('membrane-gb', nsbatwm.gaussian_blur, 'membrane', sigma=1)\nwf.set('membrane-threshold', nsbatwm.threshold_otsu, 'membrane-gb')\nwf.set('membrane-label', nsbatwm.label, 'membrane-threshold')\n\nwf.set('nucleus-gb', nsbatwm.gaussian_blur, 'nucleus', sigma=1)\nwf.set('nucleus-threshold', nsbatwm.threshold_otsu, 'nucleus-gb')\nwf.set('nucleus-label', nsbatwm.label, 'nucleus-threshold')\n\nsave_workflow('cpu_workflow-2roots-2leafs.yaml', wf)\n</pre> wf = Workflow()  wf.set('membrane-gb', nsbatwm.gaussian_blur, 'membrane', sigma=1) wf.set('membrane-threshold', nsbatwm.threshold_otsu, 'membrane-gb') wf.set('membrane-label', nsbatwm.label, 'membrane-threshold')  wf.set('nucleus-gb', nsbatwm.gaussian_blur, 'nucleus', sigma=1) wf.set('nucleus-threshold', nsbatwm.threshold_otsu, 'nucleus-gb') wf.set('nucleus-label', nsbatwm.label, 'nucleus-threshold')  save_workflow('cpu_workflow-2roots-2leafs.yaml', wf)  In\u00a0[\u00a0]: Copied! <pre>wf = load_workflow('cpu_workflow-2roots-2leafs.yaml')\n\nimg = nImage(r'images\\cells3d2ch.tiff')\nmembrane = img.get_image_data('TCZYX', C=0)\nmembrane = np.squeeze(membrane)\n\nnuclei = img.get_image_data('TCZYX', C=1)\nnuclei = np.squeeze(nuclei)\n\nwf.set('membrane', membrane)\nwf.set('nucleus', nuclei)\nmembrane_label = wf.get('nucleus-label')\n\nstackview.imshow(membrane_label)\n</pre> wf = load_workflow('cpu_workflow-2roots-2leafs.yaml')  img = nImage(r'images\\cells3d2ch.tiff') membrane = img.get_image_data('TCZYX', C=0) membrane = np.squeeze(membrane)  nuclei = img.get_image_data('TCZYX', C=1) nuclei = np.squeeze(nuclei)  wf.set('membrane', membrane) wf.set('nucleus', nuclei) membrane_label = wf.get('nucleus-label')  stackview.imshow(membrane_label)"},{"location":"examples/workflow/workflow_scripting/#scripting-a-workflow","title":"Scripting a Workflow\u00b6","text":""},{"location":"examples/workflow/workflow_widget/","title":"Workflow Widget","text":""},{"location":"examples/workflow/workflow_widget/#workflow-widget","title":"Workflow Widget\u00b6","text":"<p>The napari-ndev Workflow widgets</p>"},{"location":"tutorial/00_setup/","title":"1) Tutorial Setup","text":""},{"location":"tutorial/00_setup/#setup-for-tutorial","title":"Setup for Tutorial","text":""},{"location":"tutorial/00_setup/#installation-of-napari-and-napari-ndev","title":"Installation of napari and napari-ndev","text":"<p>You have two options to download <code>napari</code> and <code>napari-ndev</code>. For users unfamiliar with using the command line and python, I would recommend following the instructions to install from UI at Beginner Setup.</p> <p>If you are familiar with python, then I would recommend creating a new environment and to do a fresh installation with <code>napari-ndev[all]</code>. Further details available in Installation.</p>"},{"location":"tutorial/00_setup/#download-tutorial-images-and-files","title":"Download Tutorial Images and Files","text":""},{"location":"tutorial/00_setup/#cellpainting-images","title":"CellPainting Images","text":"<p>Download Images for Example Pipeline Then, extract the files in the ZIP folder. The images come from the Broad Bioimage Benchmark Collection. Investigate the link for the description of the images.</p> <p>Scale: 0.656um/pixel</p> <p>Channels:</p> <ol> <li>Hoescht 33342 (nuclei)</li> <li>con A (endoplasmic reticulum)</li> <li>SYTO 14 (nucleic acids: nucleoli, cytoplasmic RNA)</li> <li>WGA + phalloidin (plasma membrane, golgi, and actin)</li> <li>MitoTracker Deep Red (mitochondria)</li> </ol> <p></p>"},{"location":"tutorial/00_setup/#primaryneuron-images","title":"PrimaryNeuron Images","text":"<p>Download Images for Easy Machine Learning Tutorial</p> <p>These images come from my own work at the University of Minnesota in the Thomas Bastian lab. The primary neurons are derived from embryonic mouse brains, and grown for a few days in a dish. The goal is to study morphology and iron homeostasis as the neurons develop over time in conditions of iron deficiency. The images available in the tutorial are extracted from multi-scene CZI files (each original file has over 100 scenes) using the <code>Image Utilities</code> widget. Metadata from the CZI files was correct, so the widget automatically passes this downstream without any user input.</p> <p>Scale: 0.1241um/pixel</p> <p>Channels:</p> <ol> <li>AF647 - NCOA4 / nuclear coactivator 4 (a protein known to target ferritin for degradation)</li> <li>AF568 - Ferritin (the iron storage protein)</li> <li>AF488 - Phalloidin (stains actin filaments)</li> <li>DAPI (nuclei)</li> <li>Oblique (brightfield; not always present, which is ok)</li> </ol> <p></p>"},{"location":"tutorial/00_setup/#neuralprogenitor-images","title":"NeuralProgenitor Images","text":"<p>Download Images for Building a Pipeline Tutorial</p> <p>These images come from the Zhe Chen lab at the University of Minnesota. These come from a microscope that very poorly saves the images: the images are forced to be saved as RGB (dspite having only one channel in each image) and improper scaling metadata. The images available in this tutorial have already been concatenated and the metadata applied using the <code>Image Utilities</code>.</p> <p>Pax6 - Green; Tbr2 - Magenta</p> <p>Scale: 0.7548um/pixel</p> <p>Channels:</p> <ol> <li>PAX6 (a nuclear transcription factor identifying radial glia)</li> <li>PAX6-2 (a duplicate of PAX6, due to the way the microscope saves images)</li> <li>TBR2 (a nuclear transcription factor identifying intermediate progenitor cells)</li> </ol> <p></p>"},{"location":"tutorial/01_example_pipeline/","title":"2) Example Pipeline","text":""},{"location":"tutorial/01_example_pipeline/#example-pipeline-tutorial","title":"Example Pipeline Tutorial","text":"<p>The goal of this example pipeline is to get the user familiar with working with <code>napari-ndev</code> for batch processing and reproducibility (view <code>Image Utilities</code> and <code>Workflow Widget</code>). In addition, this example pipeline thoroughly explains the <code>Measure Widget</code>, since this is a shared use across many pipelines.</p> <p>This Example Pipeline does not cover how <code>napari-ndev</code> is used for high-throughput annotations, the machine learning tools (<code>APOC Widget</code>), and designing your own workflows. This information will instead be covered in the interactive tutorials that follow.</p>"},{"location":"tutorial/01_example_pipeline/#image-utilities","title":"Image Utilities","text":"<p>We are going to start with the <code>Image Utilities</code> widget in order to concatenate the CellPainting images. This will show a common use of the Image Utilities plugin, wherein various file formats can be managed and saved in to a common OME-TIFF format, including channel names and physical pixel scaling.</p> <p></p> <ol> <li><code>Choose Directory</code> selects where images will be saved.</li> <li><code>Select files</code> individual or multiple files can be selected. Select the first 5 images (representing the 5 channels of 1 image).</li> <li> <p><code>Metadata</code> dropdown. We will add in names to save the channels with, according to information that is useful. This could be the fluorophore (e.g. Hoescht 33342) or other identifying information (e.g. nuclei).</p> <ol> <li><code>Channel Name(s)</code>: copy and paste <code>['H33342', 'conA', 'SYTO14', 'WGA_Phall', 'MitoTDR']</code>. The format you want to use is a list <code>[]</code> of strings <code>'a','b','etc.'</code></li> <li><code>Scale, ZYX</code>. Set Y and X to <code>0.656</code>. Z will be ignored since images are 2D.</li> </ol> </li> <li> <p><code>Batch Concat.</code> Pressing this button will iterate through all files in the folder, selecting them in groups of 5 (i.e. the number of original files selected) and then saving them with the above parameters.</p> </li> </ol>"},{"location":"tutorial/01_example_pipeline/#investigate-the-images","title":"Investigate the images","text":"<p>If you want to investigate the raw images press <code>Open File(s)</code> this will open the original images with their known scale <code>(1,1,1)</code>. Each image will open as grayscale, and will not be layered.</p> <p>Now, investigate your concatenated images. Go to <code>Select Files</code> and find the folder <code>ConcatenatedImages</code> inside the <code>Choose Directory</code> previously chosen. Select the first image and <code>Open File(s)</code>. This time, the images will be open to the scale we set <code>(0,0.656,0.656)</code> and with a default layering and pseudo-coloring. This is how all images get passed down throughout the plugin.</p>"},{"location":"tutorial/01_example_pipeline/#example-workflow","title":"Example workflow","text":"<p>Once images are in a format that is helpful for analysis, we can proceed with other widgets. This does mean that some images do not need to be processed with the <code>Image Utilities</code> Widget; for example, some microscopes properly incorporate scale and channel names into the image metadata. For this tutorial, we are going to use the <code>Workflow Widget</code> to pre-process, segment, and label features of the image with a pre-made custom workflow file (see <code>cellpainting\\scripting_workflow.ipynb</code> to see how). The intent of the <code>Workflow Widget</code> is to easily reproduce This custom workflow was designed initially with the <code>napari-assistant</code> which will be explored further in the following tutorial sections.</p> <p>The goal for this workflow is to segment the nucleus, cell area (based on a voronoi tessellation of the nuclei), cytoplasm (cell area - nucleus), and the nucleoli. We will later measure the properties of these objects using the <code>Measure Widget</code>.</p> <p></p>"},{"location":"tutorial/01_example_pipeline/#using-the-workflow-widget-for-batch-processing","title":"Using the Workflow Widget for Batch Processing","text":"<ol> <li><code>Image Directory</code> choose the <code>ConcatenatedImages</code> found in the previous parent folder.\\</li> <li><code>Result Directory</code> create a folder to save the output images into.</li> <li><code>Workflow File</code> navigate to <code>scripted_cellpainting_workflow.yaml</code></li> </ol> <p>Now, you will now see the UI automatically update to show the <code>roots</code> (input images of the Workflow file). Furthermore, these <code>roots</code> will be populated by the channel names of the images in the chosen directory. In this workflow there are three root images required: (1) <code>Root 0: cyto_membrane</code> is <code>WGA_Phall</code>, (2) <code>Root 1: nuclei</code> is <code>H33342</code>, and (3) <code>Root 2: nucleoli</code> is <code>SYTO14</code>.</p> <p></p> <p>Next, switch to the <code>Tasks</code> tab. In this tab, the <code>leaves</code> or workflow tasks that sit at the terminals of task tree are automatically selected. However, we are also interested in visualizing the nuclei. So, hold control or command on your keyboard and also click <code>nuclei-labels</code> to add this task to the batch workflow. If all workflow tasks you are interested in are represented as <code>leaves</code> than you can even skip this tab!</p> <p></p> <p>Finally, press <code>Batch Workflow</code>. The <code>Image Directory</code> will be iterated through with the workflow. The Progress Bar will show updates and a log file will be saved to show the input parameters and progress of the batch processing, including any possible errors.</p>"},{"location":"tutorial/01_example_pipeline/#workflow-notes","title":"Workflow notes","text":"<p>Just as we selected an additional task for the workflow, any number of tasks can be acquired from the workflow and if <code>Keep Original Images</code> is checked, these will also be saved in the resulting batch processed images. As such, the workflow widget can also be used to easily visualize intermediate steps of the Workflow to investigate how something was achieved and share that information. Below, napari is showing every original channel and every task in this workflow as a grid in napari; all of this is saved into one single file.</p> <p></p> <p>Coming Soon: the ability to use layers in the workflow as roots to do single image Workflows and adding them into napari immediately!</p>"},{"location":"tutorial/01_example_pipeline/#measure-widget","title":"Measure Widget","text":"<p>The <code>Measure Widget</code> provides the ability to measure images in batch, group important information, and even utilize metadata to map sample treatments and conditions. This widget is the newest addition the <code>napari-ndev</code>, in part because it has taken me a long time to conceptualize how to make image measurements accessible in batch, so I am particularly looking for usage feedback. For detailed usage instructions see the <code>Measure Widget</code> Example.</p>"},{"location":"tutorial/01_example_pipeline/#how-measuring-in-python-generally-works","title":"How measuring in Python generally works","text":"<p>It is often most helpful to represent a segmented image as 'labels'. Labels (including the <code>Labels Layer</code> in napari) have a pseudocolor scheme where each label (i.e. object) has a specific value, and that value is represented by a color. When these labels are then measured, each label object is measured independently and represented in one row. With few objects of interest in low-throughput processes, this can make sense, but, a label image with 100 objects will result in a spreadsheet with 100 rows. Accordingly, even measuring 10 images with 100 objects each leads to 1000 rows. To many scientists, these are both small object numbers and small image numbers, so you can imagine how quickly and easily datasets can be in the hundreds of thousands or millions of rows.</p> <p>Furthermore, many many properties of images can be labeled, from area (which is scaled properly throughout this plugin to real units), to perimeter, to solidity, to sphericity. Thus, measuring label properties in Python generally requires knowledge of python to make sense of this long multi-variate data. Especially when it comes to grouping data by treatments or doing counts or other aggregating functions on any measurement of the labels.</p> <p>The <code>Measure Widget</code> seeks to address the most common usability cases for high-throughput analyses by providing human readable outputs. Furthermore, treatment metadata mapping can easily be shared from a more advanced researcher to a novice, for reproducibility of more involved analyses.</p>"},{"location":"tutorial/01_example_pipeline/#initial-batch-measurement-with-the-widget","title":"Initial Batch Measurement with the Widget","text":"<ol> <li><code>Label Directory</code>: Select the directory containing the Labels you desire to measure -- in this case choose the directory from the <code>Workflow Widget</code>. This image file can contain any number of labels (or non-labels, but those should not be measured). Channels will populate both <code>Label Image</code> select and <code>Intensity Images</code>.</li> <li><code>Image Directory</code>: An Optional directory -- choose the <code>ConcatenatedImages</code> directory to populate the original channel images to the <code>Intensity Images</code> select box.</li> <li><code>Region Directory</code>: Another Optional directory intended for 'ROI'/Region of Interest labels -- not used for this pipeline.</li> <li><code>Label image</code>: Using multi-selection, select <code>cell-labels</code>, <code>cyto-labels</code>, and <code>nuclei-labels</code>. We will measure each object in each image.</li> <li><code>Intensity images</code>: Using multi-selection, select <code>nucleoli-labels</code> (to measure the number of nucleoli inside the label), <code>conA</code> and <code>mitoTDR</code> (to measure the underlying intensity of the channel on the label).</li> <li><code>Region Props</code>. This is a list of the measurements for each label. For this example, at least select <code>label</code>, <code>area</code>, <code>intensity_mean</code> and <code>solidity</code>. <code>label</code> is the identity, and is recommended to always be checked. Otherwise you can measure shape features like area, eccentricity, and solidity or you can measure intensity features like the mean, max, min, etc. Note, that measuring something like the <code>intensity max</code> of an intensity image that represents an ROI serves as a means to identify if it is inside (i.e. the value of the ROI) or outside (i.e. 0) the region.</li> <li>At this point, you could hit the Measure button and it will measure all label channels in each image in batch. However, for this example we also want to add some identification and treatment data to the output. This example data comes from wells with no treatment, so we will generate some ourselves to explain the concept, but this should be straightforward enough to apply to your own data. To use the <code>ID Regex</code> and <code>Tx Map</code> tags we use dictionaries of key: value pairs where the key becomes the column name, and the value contains the regular expression to search for.</li> <li><code>ID Regex</code> tab. This dictionary extracts information from the filename with regular expression patterns. These data all come from <code>plate1</code> but if we had multiple plates we could extract the plate number with the following regex <code>r'(plate\\d{1,2})-'</code> whatever is inside the <code>()</code> is considered the 'group' that gets returned. In this case we can provide the dictionary to return the identifying number of the plate and the well position. We specifically need the well position in order to map it to the treatment map. Copy and paste this into <code>ID Regex</code></li> </ol> <pre><code>{\n    'plate': r'plate(\\d{1,3})_',\n    'well': r'_(\\w+?)_site',\n    'site': r'_site(\\d{1,3})_',\n}\n</code></pre> <ol> <li><code>Tx Map</code> tab. This dictionary maps well positions to an overall platemap. This time, the key remains the column identification, but then another dictionary is used to map the treatments inside, see below for example. The platemap is expected to be of standard configuration, but can include wells that are not imaged. First press 'Update Treatment ID Choices' to use the previous regex for Well ID. Select <code>well</code> for <code>Treatment ID</code> and <code>384</code> for <code>Number of Wells</code>. We are going to pretend the platemap has the following treatments:</li> </ol> <pre><code>{\n    'media': {\n        'HBSS': ['A1:C24'],\n        'DMEM': ['D1:F24'],\n    },\n    'treatment': {\n        'control': ['A12:P14'],\n        'drug': ['A15:P18'],\n    }\n}\n</code></pre> <ol> <li>Press the <code>Measure</code> button! We have all the options set to richly annotate our data with identifying info and treatments... in batch!</li> </ol>"},{"location":"tutorial/01_example_pipeline/#grouping-the-data","title":"Grouping the data","text":"<p>Navigate to the <code>Output Directory</code> and find the <code>measure_props...csv</code> for your data! You can see each measure for each label, but it's hard to read interpret this way.</p> label_name id site well plate label area intensity_mean-nucleoli-labels intensity_mean-conA intensity_mean-MitoTDR solidity row column media treatment cell-labels plate1_A14_site1_Ch1__0__plate1_A14_site1_Ch1 1 A14 1 1 1469.167104 0.7454598711189221 256.04100761570004 295.11511423550087 0.7832071576049552 A 14 HBSS control cell-labels plate1_A14_site1_Ch1__0__plate1_A14_site1_Ch1 1 A14 1 2 505.6448000000001 0.089361702 407.0757446808511 389.1506382978723 0.9767248545303407 A 14 HBSS control cell-labels plate1_A14_site1_Ch1__0__plate1_A14_site1_Ch1 1 A14 1 3 336.092416 1.2586427656850192 233.87580025608196 326.2509603072983 0.9455205811138013 A 14 HBSS control <p>Instead, we want to group the data by useful metrics. Navigate to the <code>Grouping</code> tab. Select the output <code>measure_props...csv</code> for <code>Measured Data Path</code>; the selection is interpreted to fill the remaining information in the tab. If we include the <code>id</code> name in our grouping column, then it will summarize each individual image. If you then also select other identifying information, like site, well, plate, etc. then this information will be kept in the summarized file. Ultimately, data will be grouped by the most fine-grained group (in this case, each image, aka the <code>id</code>). So, if you wanted to just know differences between treatments you could do group only by <code>treatment</code>; caution this hides your raw data and just reduces the information to the aggregate function.</p> <p>For this pipeline, we are going to group by: id, label_name (which label channel it is), site, well, plate, media, and treatment. This will summarize the data by <code>id</code> at the finest (each file), but preserve all that metadata. Then, we keep <code>Count Column</code> set to <code>label</code> so that it counts the number of each object in the image. Finally, we are going to aggregate other measured features. Select <code>Aggregation Columns</code>: <code>intensity_mean-conA</code> (to measure the intensity of ER) and <code>intensity_mean_MitoTDR</code> (mitochondria), and <code>area</code> (to compare the size of each object). Then observe how the there are multiple <code>Aggregation Functions</code> that by default is set to <code>mean</code>.</p> <p>Next, <code>check</code> <code>Pivot Wider</code>. This will place each individual label channel in the columns, rather than replicating in rows. This is generally more human-readable and familiar for non-coding statistical work.</p> <p>Finally, press <code>Group Measurements</code> button! You now have the output dataset.</p> <p></p>"},{"location":"tutorial/01_example_pipeline/#make-observations","title":"Make observations","text":"<p>One of the best parts of summarizing your data is quickly checking for quality control. Investigate <code>measure_props...grouped.csv</code></p> <ol> <li>Do we get the same number of rows that we would expect? (hint, it should be the number of images, with how we grouped)</li> <li>Are there the same number of nuclei as cytoplasms in each image? Should there be?</li> <li>Is the intensity of a certain marker localized more to the cytoplasm or the nucleus?</li> <li>Is the are of the whole cell larger than the cytoplasm and nucleus alone? Does nucleus + cytoplasm = cell?</li> </ol> id site well plate media treatment label_count label_count.1 label_count.2 area_mean area_mean.1 area_mean.2 intensity_mean-MitoTDR_mean intensity_mean-MitoTDR_mean.1 intensity_mean-MitoTDR_mean.2 intensity_mean-conA_mean intensity_mean-conA_mean.1 intensity_mean-conA_mean.2 nan nan nan nan nan nan cell-labels cyto-labels nuclei-labels cell-labels cyto-labels nuclei-labels cell-labels cyto-labels nuclei-labels cell-labels cyto-labels nuclei-labels plate1_A14_site1_Ch1__0__plate1_A14_site1_Ch1 1 A14 1 HBSS control 79.0 79.0 79.0 1458.3978094177216 1214.6478728101267 243.74993660759498 325.2266947462635 307.7767026467916 412.45047953549573 283.3011895008876 264.05000585134076 377.55476585283094 plate1_A14_site2_Ch1__0__plate1_A14_site2_Ch1 2 A14 1 HBSS control 92.0 92.0 92.0 1270.5623624347827 1015.8361933913045 254.74487930434788 335.0570967957602 319.5738507170695 405.50202331367353 273.7174073017615 256.90458122232934 349.3471722687742 plate1_B13_site1_Ch1__0__plate1_B13_site1_Ch1 1 B13 1 HBSS control 59.0 59.0 59.0 1805.06988040678 1502.4123834576274 302.6574969491526 308.182553594441 295.40835758750995 377.9935857420644 292.5031657524073 274.4186976700165 391.6966321932782 plate1_B13_site2_Ch1__0__plate1_B13_site2_Ch1 2 B13 1 HBSS control 71.0 71.0 71.0 1558.5194041690143 1291.1595267605635 267.3598774084507 299.17016394582697 285.23325714705294 363.05445180651634 305.7340276405519 284.01603790001866 406.23380996542903 plate1_C12_site1_Ch1__0__plate1_C12_site1_Ch1 1 C12 1 HBSS control 127.0 127.0 127.0 1203.229621417323 944.5570237480316 258.67259766929135 343.52778951543183 329.7049438466621 398.5256715321834 283.1283666399538 270.7264526431119 333.90274822634825 plate1_C12_site2_Ch1__0__plate1_C12_site2_Ch1 2 C12 1 HBSS control 124.0 124.0 124.0 1166.5923096774195 921.3840805161292 245.20822916129035 348.5899736336976 331.9816108022042 415.9173276097387 287.77845139732943 273.9409912708669 343.3100275440203 plate1_D16_site1_Ch1__0__plate1_D16_site1_Ch1 1 D16 1 DMEM drug 137.0 137.0 137.0 1136.5299405547446 848.7231084379563 287.80683211678837 324.09437073563026 314.2939434267824 361.2811199284404 341.1003129872079 326.6836244548794 397.2258103183944 plate1_D16_site2_Ch1__0__plate1_D16_site2_Ch1 2 D16 1 DMEM drug 126.0 126.0 126.0 1220.6002488888892 932.7191263492065 287.88112253968256 305.43130665340715 298.09709415581307 335.4623238823955 334.3002097571851 323.0418712295005 381.9689568813829 plate1_E18_site1_Ch1__0__plate1_E18_site1_Ch1 1 E18 1 DMEM drug 147.0 147.0 147.0 1054.613018122449 800.8962803809525 253.71673774149664 345.9989260804927 335.50803523526207 383.15517282809606 296.09051609563323 283.99028924506246 339.21053128779465 plate1_E18_site2_Ch1__0__plate1_E18_site2_Ch1 2 E18 1 DMEM drug 174.0 173.0 174.0 894.9381222988507 649.1033999537573 249.56520165517242 351.5135374204032 347.28641992275476 370.0639647086021 282.8140508927973 276.74482673528496 304.67453165821416"},{"location":"tutorial/02_easy_ML/","title":"3) Easy Machine Learning","text":""},{"location":"tutorial/02_easy_ML/#easy-machine-learning","title":"Easy Machine Learning","text":"<p>The goal of this tutorial is to get a user familiar with generating annotations, workflows, and machine learning classifiers. Unlike the Example Pipeline Tutorial, this tutorial just provides raw images and hints on how to progress.</p> <p>If you investigate the <code>primaryneurons</code> images you'll notice that there variable interesting morphologies that are not easy to segment by traditional intensity based segmentation. Machine Learning fills this gap (you'll see!) that Deep Learning has yet to sort out.</p> <p>You might also be surprised when looking at some of the images that I would not recommend traditional intensity-based segmentation methods for NCOA4 and Ferritin (but would, and do, use it for DAPI). Instead, I would endorse using Machine Learning based segmentation because it is less sensitive to intensity (which is expected to be different between neurons and treatment group) and more sensitive to 'Features' of the images, which includes intensity, size, blobness, ridgeness, edgeness, etc.</p> <p>Machine Learning classifiers are trained using <code>accelerated-pixel-and-object-classifiers</code> (APOC) under the hood; the examples in the <code>apoc</code> repository are excellent!</p> <p>The <code>APOC Widget</code> can be used to Segment Objects, Classify Pixels, and Classify Objects. Furthermore, the widget can visualize custom feature sets and be applied in the viewer or in batch.</p> <p>To train Machine Learning classifiers, you need to provide some sample, sparse annotation that the classifier can evaluate as the class of pixel you are interested. A typical convention is to use label 1 for background, and subsequent labels with an increasing number. This is different from Deep Learning that requires complete and accurate annotations. In comparison, Machine Learning is much more lenient.</p> <p>Overall, new Machine Learning classifiers can be evaluated within seconds and batch training can be accomplished in minutes... with great results!</p> <p>The skills practiced in this will be used on relatively small, 2D images; however, things are intended to generally transfer to both 3D and higher dimensional datasets.</p> <p>Coming very soon object classification (and not just segmentation) to the <code>APOC Widget</code></p>"},{"location":"tutorial/02_easy_ML/#sparse-annotation-with-image-utilities","title":"Sparse annotation with Image Utilities","text":"<p>One strength of <code>napari-ndev</code> is the ability to quickly annotate images and save them, while maintaining helpful metadata to pair the images up for future processing. In <code>napari</code> annotations can be made using <code>labels</code> or <code>shapes</code>. Shapes has a current weakness in that it cannot save <code>images</code>, so <code>napari-ndev</code> converts shapes to <code>labels</code> so that they match the image format. For this tutorial, we want to use the <code>labels</code> feature to 'draw' on annotations.</p> <ol> <li>Load in one of the primary neuron images in <code>ExtractedScenes</code> using the <code>Image Utilities</code> widget.</li> <li>Add a Labels layer by clicking the <code>tag</code> icon (the third button above the layer list)</li> <li>Click the <code>Paintbrush</code> button in the <code>layer controls</code>.</li> <li>Click and drag on the image to draw annotations.</li> <li>Draw background labels with the original label (1)</li> <li>Draw signal labels, trying to label 1-10% of the signal in the image, and with a variety of features of the target signal.</li> <li>Press <code>Save selected Layers</code> to save the annotation for future use! It will save into a <code>Labels</code> folder in the directory.</li> </ol> <p></p>"},{"location":"tutorial/02_easy_ML/#generating-a-machine-learning-classifier","title":"Generating a Machine Learning Classifier","text":"<ol> <li>Open the <code>APOC Widget</code></li> <li>Select a classifier file with the first button. Unfortunately, you will need to right click in your file explorer, create a new file (I usually create a .txt) and then rename it to something like <code>classifier.cl</code>. Your operarting system will prompt you that you are changing the extension of the file which could break the file, but this is ok since it is a brand new file. Select this file and hit the open button.</li> <li>Use the default ObjectSegmenter. The number of <code>forests</code> is the number of total iterations of the classifier and the number of <code>trees</code> is the number of decisions that the Random Forest Classifier will make. These defaults are ok, but for more specific classifier, I would increase these values.</li> <li>You can select a pre-made feature set OR do a custom feature set. (See below)</li> <li>For now, select from the <code>feature_set</code> dropdown: <code>object_size_1_to_5_px</code></li> </ol>"},{"location":"tutorial/02_easy_ML/#trainingpredicting-using-the-viewer","title":"Training/Predicting using the viewer","text":"<p>We can train and predict with machine learning classifiers on individual images in the viewer. This is useful for initially determining a useful feature set prior to training in batch.</p> <ol> <li>Switch to the <code>Viewer</code> tab</li> <li>Select the channels that you want to use for training. For morphology, you want to select at least AF488 (phalloidin), but it may also be useful to select other signals that fill the cell. Play around!</li> <li>Press the <code>Train classifier on select layers using label</code> button. In a few seconds your classifier should be finished training!</li> <li><code>Predict using classifier on selected layers</code> button to add the results immediately to the viewer.</li> </ol> <p>As you can see, there is likely some errors that need to be corrected for:</p> <p></p>"},{"location":"tutorial/02_easy_ML/#generation-of-a-feature-set-with-apoc-widget","title":"Generation of a Feature Set with APOC Widget","text":"<p>We now want to tune up the feature set. Check out the <code>features</code> of the originally selected feature set. If you reselect the classifier file (after selecting any other arbitrary file) a new popup will appear with a table displaying the value of each feature.</p> <p>To visualize the feature set:</p> <ol> <li>Go to the <code>Custom Feature Set</code> tab. The feature set you have selected will automatically generate the <code>custom feature string</code> used.</li> <li>Select the image layer you want to visualize (in this case AF488)</li> <li>Press <code>Apply to Selected Image</code></li> <li>Either turn on/off layers to visualize. Or switch napari to <code>grid</code> mode.</li> </ol> <p></p> <p>Optionally, switch between the different feature sets to try them out. Notice how different a large pixel set is compared to a small one. Let's attempt to generate a custom feature set.</p> <ol> <li>Switch to <code>custom</code> in the <code>feature set</code> dropdown.</li> <li>In each feature you think could be interesting add values that you think might be useful, separated by commas. For example, Gaussian blur might be <code>1,2,3</code>. Look up the different features if you aren't sure what their use is!</li> <li>Press <code>Generate Feature String</code>. This will populate it for this tab, and also above in the <code>Feature String</code> used by the classifier. Neat!</li> <li>Visualize the feature set.</li> <li>Try this new feature set on the image in the viewer. You may wish to create a new classifier file (to preserve progress). If you wish to 'overwrite' the current classifier for ease of use, uncheck <code>Continue Training?</code> at the top. When this is checked, it can be used to iterate over the classifier for a 'batch-like' training experience on a previously used classifier (be cautious with this, but it is generally a helpful default value).</li> </ol>"},{"location":"tutorial/02_easy_ML/#trainingpredicting-in-batch","title":"Training/Predicting in batch","text":"<ol> <li>Now, go back and annotate and save the labels of the other file.</li> <li>Use the <code>Batch</code> tab</li> <li>Select the <code>Image Directory</code></li> <li>Select the channels to be used for training, can be multiple</li> <li>Select the <code>Label Directory</code></li> <li>Train!</li> <li>Predict!</li> </ol>"},{"location":"tutorial/03_build_pipeline/","title":"4) Build Your Own Pipeline","text":""},{"location":"tutorial/03_build_pipeline/#build-your-own-workflow","title":"Build your own Workflow","text":"<p>The goal of this tutorial is to get a user familiar with generating ROI annotations and building your own workflows. Unlike the Example Pipeline Tutorial, this tutorial just provides raw images and hints on how to progress.</p> <p>For this workflow, we will be using the <code>neuralprogenitors</code> images. Our goal is to segment the PAX6 and TBR2 channels. We also specifically want to make an ROI that is 200 microns wide on each image, and bin a specific region of the brand (the specifics beyond the scope and necessity of this tutorial). Later, we will use these labels to count only the ones inside the region of interest.</p> <p>The skills practiced in this tutorial will be used on relatively small, 2D images; however, things are intended to generally transfer to both 3D and higher dimensional datasets.</p> <p></p>"},{"location":"tutorial/03_build_pipeline/#annotating-regions-of-interest-with-image-utilities","title":"Annotating regions of interest with Image Utilities","text":"<ol> <li>Load in one of the neural progenitor images from <code>ConcatenatedImages</code> using the <code>Image Utilities</code> widget.</li> <li>Navigate in the toolbar to <code>View</code> -&gt; <code>Scale Bar</code> -&gt; <code>Scale Bar Visible</code>. Now there should be a scale bar in the bottom right</li> <li>Add a Shapes layer by clicking the <code>polygon</code> icon (the second button above the layer list)</li> <li>Click the <code>Rectangle</code> button in the <code>layer controls</code>.</li> <li>Click and drag on the image to draw a rectangle that has a 200um width.</li> <li>Select button number 5 (highlighted in blue in the screenshot) to select the shape.</li> <li>Move the shape by dragging</li> <li>Rotate the shape into an area of interest.</li> <li>Finally, with the <code>Shapes</code> layer highlighted. Click the <code>Save Selected Layers</code> button in the <code>Image Utilities Widget</code></li> </ol>"},{"location":"tutorial/03_build_pipeline/#using-the-napari-assistant-to-generate-a-workflow","title":"Using the napari-assistant to generate a workflow","text":"<ol> <li>Open the <code>napari-assistant</code> by navigating in the toolbar to <code>Plugins</code> -&gt; <code>Assistant (napari-assistant)</code></li> <li>Select the image you want to process.</li> <li>Play around with the assistant buttons that seem interesting. Play around! They are sort of logically ordered left to right, top to bottom. The label layer I have in the image is not quality segmentation. Check the goal image above.</li> <li>You can modify parameters and functions on the fly, including in previously used functions by clicking on that specific layer.</li> <li>If you need help reaching the goal (of quality segmentation of the nuclei), try out some of the hints.</li> <li>When you are satisfied with what the workflow. Click the <code>Save and load ...</code> button -&gt; <code>Export workflow to file</code> and save the .yaml file produced.</li> </ol>"},{"location":"tutorial/03_build_pipeline/#hints","title":"Hints","text":"How to label <p>You may find the functions in the <code>Label</code> button to be quite useful.</p> A very useful label function <p>Check out the voronoi_otsu_labeling function. Read the link for more info.</p> Pre-processing the images to reduce background <p>Try playing with functions in <code>remove noise</code> and <code>remove background</code> to remove some of the variability in background intensity and off-target fluorescence prior to labeling. This will make labeling more consister.</p> Cleaning up the labels <p>Perhaps you have criteria for what labels you want to keep. Check out <code>Process Labels</code> button for cleaning up things like small or large labels, or labels on the edges.</p> OK, I give up, just give me the answer <p>Something like the following should work well.</p> <ol> <li>median_sphere (pyclesperanto) with radii of 1</li> <li>top_hat_sphere (pyclesperanto) with radii of 10 (roughly the diameter of the objects)</li> <li>voronoi_otsu_label (pyclesperanto) with spot and outline sigmas of 1</li> <li>exclude_small_labels (pyclesperanto) that are smaller than 10 pixels</li> </ol>"},{"location":"tutorial/03_build_pipeline/#applying-your-workflow-in-batch-with-the-workflow-widget","title":"Applying your workflow in batch with the Workflow Widget","text":"<p>Consider the instructions for Using the Workflow Widget for Batch Processing and apply it to this workflow.</p>"},{"location":"tutorial/03_build_pipeline/#measuring-your-batch-workflow-output","title":"Measuring your batch workflow output","text":"<p>In additional to how we already learned how to use the <code>Measure Widget</code>, we can also consider additional creative possibility. In this case, we want to only count cells in our region of interest (the shape rectangle that was drawn), so we want to load this in as a <code>Region Directory</code>. Then, we want to ensure that the <code>Shape</code> is added as an <code>Intensity Image</code> and that we measure the <code>intensity_max</code> or <code>intensity_min</code>. The maximum intensity of an object if it touches the region of interest at any point will be 1. The minimum intensity of an object fully inside the ROI will be 1, since all pixels are inside the ROI. So, you can choose how you want to consider objects relative to the ROI.</p> <p>Then, when grouping the data, use the <code>intensity_max/min_Shape</code> as a grouping variable! Then, all labels with a value of 1 or 0 will be counted separately. This can be extended to multiple regions of interest, because each shape has it's own value (not immediately obvious yet in napari). We have used this to label multiple brain regions consistently in whole brain section analyses.</p> <p>Future addition: The ability to simply filter objects in the Measure Widget. This can for example be used to exclude all labels that are outside the region of interest (having a intensity value of 0 relative to the ROI), instead of having to group.</p>"},{"location":"tutorial/03_build_pipeline/#notes-on-multi-dimensional-data","title":"Notes on multi-dimensional data","text":"<p>Overall, most of the plugin should be able to handle datasets that have time, multi-channel, and 3D data. Try exploring the <code>Lund Timelapse (100MB)</code> sample data from <code>Pyclesperanto</code> in napari.</p>"},{"location":"tutorial/cellpainting/scripting_workflow/","title":"Scripting workflow","text":"In\u00a0[1]: Copied! <pre>import pyclesperanto_prototype as cle\nimport stackview\nfrom napari_workflows import Workflow\nfrom napari_workflows._io_yaml_v1 import save_workflow\n\nfrom napari_ndev import morphology, nImage\n</pre> import pyclesperanto_prototype as cle import stackview from napari_workflows import Workflow from napari_workflows._io_yaml_v1 import save_workflow  from napari_ndev import morphology, nImage  In\u00a0[2]: Copied! <pre>wf = Workflow()\n\n# label nuclei\nwf.set('nuclei-labels', cle.voronoi_otsu_labeling, 'nuclei', spot_sigma=5, outline_sigma=1)\n# voronoi diagram\nwf.set('nuclei-voronoi', cle.extend_labeling_via_voronoi, 'nuclei-labels')\n\n# label nucleoli\nwf.set('nucleoli-med', cle.median_sphere, 'nucleoli', radius_x=1, radius_y=1)\nwf.set('nucleoli-th', cle.top_hat_sphere, 'nucleoli-med', radius_x=5, radius_y=5)\nwf.set('nucleoli-labels', cle.voronoi_otsu_labeling, 'nucleoli-th', spot_sigma=1, outline_sigma=1)\n\n# label cytoplasm\nwf.set('cyto-med', cle.median_sphere, 'cyto-membrane', radius_x=1, radius_y=1)\nwf.set('cyto-thresh', cle.greater_constant, 'cyto-med', constant=200)\nwf.set('cyto-no-nuclei', cle.logical_xor, 'cyto-thresh', 'nuclei-labels')\n\n# voronoi, full cells\nwf.set('cell-labels-float', cle.multiply_images, 'nuclei-voronoi', 'cyto-thresh')\nwf.set('cell-labels', morphology.convert_float_to_int, 'cell-labels-float')\n\n# voronoi, only with cytoplasm\nwf.set('cyto-labels-float', cle.multiply_images, 'nuclei-voronoi', 'cyto-no-nuclei')\nwf.set('cyto-labels', morphology.convert_float_to_int, 'cyto-labels-float')\n\nsave_workflow('scripted_cellpainting_workflow.yaml', wf)\n</pre> wf = Workflow()  # label nuclei wf.set('nuclei-labels', cle.voronoi_otsu_labeling, 'nuclei', spot_sigma=5, outline_sigma=1) # voronoi diagram wf.set('nuclei-voronoi', cle.extend_labeling_via_voronoi, 'nuclei-labels')  # label nucleoli wf.set('nucleoli-med', cle.median_sphere, 'nucleoli', radius_x=1, radius_y=1) wf.set('nucleoli-th', cle.top_hat_sphere, 'nucleoli-med', radius_x=5, radius_y=5) wf.set('nucleoli-labels', cle.voronoi_otsu_labeling, 'nucleoli-th', spot_sigma=1, outline_sigma=1)  # label cytoplasm wf.set('cyto-med', cle.median_sphere, 'cyto-membrane', radius_x=1, radius_y=1) wf.set('cyto-thresh', cle.greater_constant, 'cyto-med', constant=200) wf.set('cyto-no-nuclei', cle.logical_xor, 'cyto-thresh', 'nuclei-labels')  # voronoi, full cells wf.set('cell-labels-float', cle.multiply_images, 'nuclei-voronoi', 'cyto-thresh') wf.set('cell-labels', morphology.convert_float_to_int, 'cell-labels-float')  # voronoi, only with cytoplasm wf.set('cyto-labels-float', cle.multiply_images, 'nuclei-voronoi', 'cyto-no-nuclei') wf.set('cyto-labels', morphology.convert_float_to_int, 'cyto-labels-float')  save_workflow('scripted_cellpainting_workflow.yaml', wf)  In\u00a0[3]: Copied! <pre>img = nImage(r'ConcatenatedImages/plate1_A14_site1_Ch1.tiff')\n\nnuclei = img.get_image_data('TCZYX', C=0).squeeze()\ncyto_membrane = img.get_image_data('TCZYX', C=3).squeeze()\nnucleoli = img.get_image_data('TCZYX', C=2).squeeze()\n\nwf.set('nuclei', nuclei)\nwf.set('cyto-membrane', cyto_membrane)\nwf.set('nucleoli', nucleoli)\n</pre> img = nImage(r'ConcatenatedImages/plate1_A14_site1_Ch1.tiff')  nuclei = img.get_image_data('TCZYX', C=0).squeeze() cyto_membrane = img.get_image_data('TCZYX', C=3).squeeze() nucleoli = img.get_image_data('TCZYX', C=2).squeeze()  wf.set('nuclei', nuclei) wf.set('cyto-membrane', cyto_membrane) wf.set('nucleoli', nucleoli) In\u00a0[4]: Copied! <pre>tasks = [\n    'nuclei-labels',\n    'nuclei-voronoi',\n    'nucleoli-labels',\n    'cell-labels',\n    'cyto-labels',\n]\n\nfor task in tasks:\n    display(task)\n    label_image = wf.get(task)\n    display(label_image.dtype)\n    stackview.imshow(label_image, labels=True)\n</pre> tasks = [     'nuclei-labels',     'nuclei-voronoi',     'nucleoli-labels',     'cell-labels',     'cyto-labels', ]  for task in tasks:     display(task)     label_image = wf.get(task)     display(label_image.dtype)     stackview.imshow(label_image, labels=True) <pre>'nuclei-labels'</pre> <pre>dtype('uint32')</pre> <pre>'nuclei-voronoi'</pre> <pre>dtype('uint32')</pre> <pre>'nucleoli-labels'</pre> <pre>dtype('uint32')</pre> <pre>'cell-labels'</pre> <pre>dtype('uint32')</pre> <pre>'cyto-labels'</pre> <pre>dtype('uint32')</pre> In\u00a0[5]: Copied! <pre>wf.get('nuclei-labels')\n</pre> wf.get('nuclei-labels') Out[5]: cle._ image shape(520,\u00a0696) dtypeuint32 size1.4 MB min0.0max79.0"}]}